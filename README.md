# ADAM-optimizer


This notebook is created at Deep Learning class, ISAE-SUPAERO. The aim of this work is to go trough main concepts of optimization techniques and to look closely at ADAM optimizator. We compare here ADAM to more 'classical' optimizers such as Stochastic Gradient Descent. We will also take a look on of of the ADAM modification algorithms - AdaMax. We will finally make empirical comparison between optimizers and think on whether it's wise or not to use such comparison 'sightlessly'. Enjoy !
