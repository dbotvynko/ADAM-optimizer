# ADAM-optimizer


This notebook is created at Deep Learning class, ISAE-SUPAERO. The aim of this worj is to go trough main concepts of optimization techniques and to look closely at ADAM optimizator. We compare here ADAM to more 'classical' optimizers such as Stochastic Gradient Descent. We will also take a look on an ADAM modification algorithm - AdaMax. We will finally make empirical comparizon between optimizers and think on whether it's wise or not to use such comparizon 'sightlessly'. Enjoy !
